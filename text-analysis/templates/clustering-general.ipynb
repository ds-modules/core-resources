{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an unsupervised machine learning method, as we are not telling the computer the groupings. Clustering is not the same as topic modeling, although clustering can yield topics. Clustering is a more restricted approach to grouping and visualizing data based on their similarity. If you only want to determine topics, a conventional LDA model will be more accurate as it allows for documents to be assigned to more than one topic. If you are looking for spatial relations and 1-1 assignment of documents to groupings, clustering will show this better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll define functions in order to collect tokenized words and stemmed words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = [x for x in tokens if x not in punctuation] #word tokenizer cuts the possessives\n",
    "    return filtered_tokens\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    stems = [stemmer.stem(x) for x in tokenize_only(text)]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we collect these from our paragraphs, this is only necessary to map out our data points after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "\n",
    "for d in docs:\n",
    "    totalvocab_stemmed.extend(tokenize_and_stem(d))\n",
    "    totalvocab_tokenized.extend(tokenize_only(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data frame will map tokenized words to stemmed words, recalling our work with pandas in Day 3 of the introductory series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print (vocab_frame.shape[0])\n",
    "print (vocab_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a tfidf, *term freqency inverse document frequency*, matrix. A tfidf takes into account the frequency of a word in the entire corpus, and offsets it based on its frequency among documents (more here: https://en.wikipedia.org/wiki/Tfâ€“idf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters, max_df is maximum occurence in docs of word, min_df is opposite\n",
    "#use .8 max to eliminate more common words, lower .2 looking for unique but not proper nouns\n",
    "#use inverse document frequency, give more weight to rare words\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=20000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,5))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(docs) #fit the vectorizer to the docs\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfidf_matrix maps every document with every term meeting the vectorizer paramerters, and assigns a weight based on the term's frequency amongst all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need the words from the vector, these are essentially most influential words taking both document frequency and corpus frequency into account, we will eventually assign them to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print (terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot our clusters in a 2D plane, we'll want to calculate the distance between any two given docs via cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix) #creates high dimensional object\n",
    "print (dist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll start the actual clustering. The algorithm assigns each observation to the cluster whose mean yields the least within-cluster sum of squares, essentially the nearest mean. This iterates until the mean no longer changes.\n",
    "\n",
    "But how do we know the number of clusters? This is a highly contentious matter. There are various methods (https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cnum = 6\n",
    "km = KMeans(init='k-means++', n_clusters=cnum, n_init=10, random_state=10)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist() #assigns each paragraph to the respective cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do a form of topic modelling by printing the words characterizing the clusters we made, the words are those closest to the centroid of the cluster, extracted from the vocab data frame, indexed by their position within the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sort cluster centers by proximity to centroid, and grabs the index to iterate through below\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "cents_words = [] #to collect words for chart legend\n",
    "\n",
    "for i in range(cnum): #numer of clusters\n",
    "    cent = []\n",
    "    print(\"Cluster %i words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :7]: #ind is index, replace 7 with n words per cluster, how many to choose from centroid\n",
    "        a = vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0] #indexing term in dataframe\n",
    "        cent.append(a)\n",
    "        \n",
    "        print(' %a' % a, end=',')\n",
    "        \n",
    "    cents_words.append(cent)\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see which doc was assigned to which cluster, we'll zip our titles and clusters together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (list(zip(titles,clusters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two dimensional scaling must be applied for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state = 10)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples), based on distances\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1] #grabs x and y coordinates from pos (numpy array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define colors and labels for plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "cluster_colors = {}\n",
    "cluster_names ={}\n",
    "\n",
    "cols = [\"b\", \"g\", \"r\", \"c\", \"m\",\"y\"]\n",
    "\n",
    "for i in range(cnum): # for each cluster\n",
    "    #cluster_colors[i] = \"#%06x\" % random.randint(0, 0xFFFFFF) #random hexadecimal color\n",
    "    cluster_colors[i] = cols[i]\n",
    "    cluster_names[i] = ' '.join(cents_words[i][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size, subplots yields a tuple of figure and axes, hence the two assignments\n",
    "ax.margins(0.15) # Optional, just adds 10% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, #marker size\n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none') #'marker edge color'\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the paragraph number\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=7)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also cluster hierarchically via Ward's method (https://en.wikipedia.org/wiki/Ward%27s_method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 40)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=titles);\n",
    "\n",
    "plt.tick_params(labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "# plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
